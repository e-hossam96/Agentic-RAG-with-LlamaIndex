{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will experiment RAG with multi-document agent.\n",
    "\n",
    "- Define a reader to read the `pdf` sample file [AraGPT2](./data/aragpt2.pdf) paper.\n",
    "- Define a `splitter` to process the texts of the document.\n",
    "- Set the LLM embedding and generation model ids.\n",
    "- Create the engines from the Indexes and define a tool wrapper around them.\n",
    "- Create Index for tool objects.\n",
    "- Define the agent worker and agent runner that utilize memory.\n",
    "- Excute the multi-docs agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variables\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some constants\n",
    "GENERATION_MODEL_ID = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL_ID = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=GENERATION_MODEL_ID)\n",
    "Settings.embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "\n",
    "def get_nodes(file_paths: list[str]) -> dict[str, TextNode]:\n",
    "    \"\"\"Extract text nodes from documents.\n",
    "    \n",
    "    inputs:\n",
    "        file_paths (list[str]): paths to pdf files. must be unique.\n",
    "    returns:\n",
    "        nodes_dict (dict[str, TextNode]): mapping of file paths to nodes.\n",
    "    \"\"\"\n",
    "    nodes_dict = {file_path.split(\"/\")[-1].split(\".\")[0]: [] for file_path in file_paths}\n",
    "    documents_reader = SimpleDirectoryReader(input_files=file_paths)\n",
    "    documents = documents_reader.load_data()\n",
    "    sentence_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "    for node in nodes:\n",
    "        nodes_dict[node.metadata[\"file_name\"].split(\".\")[0]].append(node)\n",
    "    return nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/arabert.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/gpt2.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/aragpt2.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/camel_parser.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/camel_bert.pdf'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'data/arabert.pdf'\u001b[0m, \u001b[32m'data/gpt2.pdf'\u001b[0m, \u001b[32m'data/aragpt2.pdf'\u001b[0m, \u001b[32m'data/camel_parser.pdf'\u001b[0m, \u001b[32m'data/camel_bert.pdf'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "file_paths = glob.glob(\"data/*\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = get_nodes(file_paths=file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dict_keys</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'arabert'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'aragpt2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'camel_parser'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'camel_bert'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'arabert'\u001b[0m, \u001b[32m'gpt2'\u001b[0m, \u001b[32m'aragpt2'\u001b[0m, \u001b[32m'camel_parser'\u001b[0m, \u001b[32m'camel_bert'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nodes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Vector Search and Summary Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "\n",
    "def get_tools_from_nodes(\n",
    "    nodes: dict[str, TextNode]\n",
    ") -> dict[str, list[QueryEngineTool]]:\n",
    "    \"\"\"Define query engine tools from nodes dictionary.\n",
    "\n",
    "    inputs:\n",
    "        nodes (dict[str, TextNode]): text nodes for each document by name.\n",
    "    returns:\n",
    "        tools_dict (dict[str, list[QueryEngineTool]]): tools for each document by name.\n",
    "    \"\"\"\n",
    "    tools_dict = {file_name: [] for file_name in nodes.keys()}\n",
    "    for file_name, text_nodes in nodes.items():\n",
    "        print(f\"Creating tools for file: {file_name}\")\n",
    "        # define vector tool\n",
    "        vector_index = VectorStoreIndex(nodes=text_nodes)\n",
    "        vector_engine = vector_index.as_query_engine()\n",
    "        vector_metadata = ToolMetadata(\n",
    "            name=f\"vector_tool_for_{file_name}\",\n",
    "            description=f\"Useful for retrieving specific context from the {file_name} paper.\",\n",
    "        )\n",
    "        vector_tool = QueryEngineTool(\n",
    "            query_engine=vector_engine, metadata=vector_metadata\n",
    "        )\n",
    "        # define summary tool\n",
    "        summary_index = SummaryIndex(nodes=text_nodes)\n",
    "        summary_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "        summary_metadata = ToolMetadata(\n",
    "            name=f\"summary_tool_for_{file_name}\",\n",
    "            description=f\"Useful for summarization questions related to the {file_name} paper.\",\n",
    "        )\n",
    "        summary_tool = QueryEngineTool(\n",
    "            query_engine=summary_engine, metadata=summary_metadata\n",
    "        )\n",
    "        tools_dict[file_name].extend([vector_tool, summary_tool])\n",
    "    return tools_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: arabert\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: arabert\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: gpt2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: gpt2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: aragpt2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: aragpt2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: camel_parser\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: camel_parser\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: camel_bert\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: camel_bert\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tools = get_tools_from_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">llama_index.core.tools.query_engine.QueryEngineTool</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f5022b1cc50</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mllama_index.core.tools.query_engine.QueryEngineTool\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f5022b1cc50\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tools[\"arabert\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Objects Vector Search Tool\n",
    "\n",
    "This will do vector search over the tools we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.objects import ObjectIndex, ObjectRetriever\n",
    "\n",
    "\n",
    "def get_object_tool(tools: dict[str, list[QueryEngineTool]]) -> ObjectRetriever:\n",
    "    \"\"\"Define tools query engine tool.\n",
    "\n",
    "    inputs:\n",
    "        tools (dict[str, list[QueryEngineTool]]): tools for each document by name.\n",
    "    returns:\n",
    "        tools_retriever (ObjectRetriever): tools object retriever.\n",
    "    \"\"\"\n",
    "    tools_index = ObjectIndex.from_objects(\n",
    "        objects=[tl for _, tls in tools.items() for tl in tls], index_cls=VectorStoreIndex\n",
    "    )\n",
    "    tools_retriever = tools_index.as_retriever(similarity_top_k=4)\n",
    "    return tools_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">llama_index.core.objects.base.ObjectRetriever</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f5022b200d0</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mllama_index.core.objects.base.ObjectRetriever\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f5022b200d0\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tools_retriever = get_object_tool(tools)\n",
    "print(tools_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Worker and Runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are an agent designed to answer queries over a set of given papers. Please always use the tools provided to \n",
       "answer a question. Do not rely on prior knowledge.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are an agent designed to answer queries over a set of given papers. Please always use the tools provided to \n",
       "answer a question. Do not rely on prior knowledge.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sys_prompt = \"\"\"\\\n",
    "You are an agent designed to answer queries over a set of \\\n",
    "given papers. Please always use the tools provided to answer \\\n",
    "a question. Do not rely on prior knowledge.\\\n",
    "\"\"\"\n",
    "\n",
    "print(sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import (\n",
    "    AgentRunner,\n",
    "    FunctionCallingAgentWorker,\n",
    ")\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=tools_retriever,\n",
    "    system_prompt=sys_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare between the AraBERT and AraGPT2 models on a high level.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_for_aragpt2 with args: {\"input\": \"high level comparison between AraBERT and AraGPT2\"}\n",
      "=== Function Output ===\n",
      "AraBERT and AraGPT2 are both transformer-based models designed for the Arabic language, but they serve different purposes and are built on distinct methodologies. \n",
      "\n",
      "AraBERT primarily focuses on natural language understanding (NLU) tasks and is pre-trained using the Masked Language Modeling (MLM) approach. This model is optimized for tasks such as sentence classification and entity recognition, leveraging a masked token prediction strategy to understand context and relationships within the text.\n",
      "\n",
      "In contrast, AraGPT2 is designed for language generation, specifically causal language modeling. It is trained from scratch on a large corpus of Arabic text, enabling it to generate coherent and contextually relevant text. AraGPT2 comes in multiple size variants, with the largest model containing 1.46 billion parameters, making it the largest Arabic language model available. Its training methodology allows it to excel in tasks like synthetic news generation and zero-shot question answering.\n",
      "\n",
      "While AraBERT is tailored for understanding and processing Arabic text, AraGPT2 is focused on generating text, showcasing the advancements in Arabic NLP through its ability to produce human-like articles and responses.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_for_gpt2 with args: {\"input\": \"high level comparison between AraBERT and AraGPT2\"}\n",
      "=== Function Output ===\n",
      "A high-level comparison between AraBERT and AraGPT2 can be drawn based on their underlying architectures and intended applications. AraBERT is designed primarily for understanding and processing Arabic text, focusing on tasks such as classification and named entity recognition. It leverages a transformer architecture similar to BERT, which is optimized for bidirectional context understanding.\n",
      "\n",
      "On the other hand, AraGPT2 is built on the GPT-2 architecture, which is tailored for generating coherent and contextually relevant text. It excels in tasks that require text generation, such as conversational agents and creative writing, by predicting the next word in a sequence based on the preceding context.\n",
      "\n",
      "While AraBERT is more suited for comprehension tasks that require a deep understanding of the text, AraGPT2 is optimized for generating text and maintaining context over longer passages. Each model serves distinct purposes within the realm of natural language processing for Arabic, reflecting their respective strengths in understanding versus generation.\n",
      "=== LLM Response ===\n",
      "AraBERT and AraGPT2 are both transformer-based models tailored for the Arabic language, but they have different focuses and methodologies:\n",
      "\n",
      "### AraBERT:\n",
      "- **Purpose**: Primarily designed for natural language understanding (NLU) tasks.\n",
      "- **Training Methodology**: Pre-trained using the Masked Language Modeling (MLM) approach, which involves predicting masked tokens in a sentence.\n",
      "- **Applications**: Optimized for tasks such as sentence classification and named entity recognition, leveraging its ability to understand context and relationships within the text.\n",
      "- **Architecture**: Similar to BERT, it utilizes a bidirectional context understanding, making it effective for comprehension tasks.\n",
      "\n",
      "### AraGPT2:\n",
      "- **Purpose**: Focused on language generation, specifically causal language modeling.\n",
      "- **Training Methodology**: Trained from scratch on a large corpus of Arabic text, enabling it to generate coherent and contextually relevant text.\n",
      "- **Applications**: Excels in tasks like synthetic news generation, conversational agents, and zero-shot question answering. It can produce human-like articles and responses.\n",
      "- **Architecture**: Built on the GPT-2 architecture, which is optimized for predicting the next word in a sequence based on preceding context.\n",
      "\n",
      "### Summary:\n",
      "- **AraBERT** is tailored for understanding and processing Arabic text, making it suitable for comprehension tasks.\n",
      "- **AraGPT2** is optimized for generating text and maintaining context over longer passages, reflecting its strengths in creative writing and conversational applications.\n",
      "\n",
      "Each model serves distinct purposes within Arabic natural language processing, showcasing their respective strengths in understanding versus generation.\n"
     ]
    }
   ],
   "source": [
    "query = \"Compare between the AraBERT and AraGPT2 models on a high level.\"\n",
    "\n",
    "\n",
    "response = agent.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">AraBERT and AraGPT2 are both transformer-based models tailored for the Arabic language, but they have different \n",
       "focuses and methodologies:\n",
       "\n",
       "### AraBERT:\n",
       "- **Purpose**: Primarily designed for natural language understanding <span style=\"font-weight: bold\">(</span>NLU<span style=\"font-weight: bold\">)</span> tasks.\n",
       "- **Training Methodology**: Pre-trained using the Masked Language Modeling <span style=\"font-weight: bold\">(</span>MLM<span style=\"font-weight: bold\">)</span> approach, which involves \n",
       "predicting masked tokens in a sentence.\n",
       "- **Applications**: Optimized for tasks such as sentence classification and named entity recognition, leveraging \n",
       "its ability to understand context and relationships within the text.\n",
       "- **Architecture**: Similar to BERT, it utilizes a bidirectional context understanding, making it effective for \n",
       "comprehension tasks.\n",
       "\n",
       "### AraGPT2:\n",
       "- **Purpose**: Focused on language generation, specifically causal language modeling.\n",
       "- **Training Methodology**: Trained from scratch on a large corpus of Arabic text, enabling it to generate coherent\n",
       "and contextually relevant text.\n",
       "- **Applications**: Excels in tasks like synthetic news generation, conversational agents, and zero-shot question \n",
       "answering. It can produce human-like articles and responses.\n",
       "- **Architecture**: Built on the GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> architecture, which is optimized for predicting the next word in a sequence \n",
       "based on preceding context.\n",
       "\n",
       "### Summary:\n",
       "- **AraBERT** is tailored for understanding and processing Arabic text, making it suitable for comprehension tasks.\n",
       "- **AraGPT2** is optimized for generating text and maintaining context over longer passages, reflecting its \n",
       "strengths in creative writing and conversational applications.\n",
       "\n",
       "Each model serves distinct purposes within Arabic natural language processing, showcasing their respective \n",
       "strengths in understanding versus generation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "AraBERT and AraGPT2 are both transformer-based models tailored for the Arabic language, but they have different \n",
       "focuses and methodologies:\n",
       "\n",
       "### AraBERT:\n",
       "- **Purpose**: Primarily designed for natural language understanding \u001b[1m(\u001b[0mNLU\u001b[1m)\u001b[0m tasks.\n",
       "- **Training Methodology**: Pre-trained using the Masked Language Modeling \u001b[1m(\u001b[0mMLM\u001b[1m)\u001b[0m approach, which involves \n",
       "predicting masked tokens in a sentence.\n",
       "- **Applications**: Optimized for tasks such as sentence classification and named entity recognition, leveraging \n",
       "its ability to understand context and relationships within the text.\n",
       "- **Architecture**: Similar to BERT, it utilizes a bidirectional context understanding, making it effective for \n",
       "comprehension tasks.\n",
       "\n",
       "### AraGPT2:\n",
       "- **Purpose**: Focused on language generation, specifically causal language modeling.\n",
       "- **Training Methodology**: Trained from scratch on a large corpus of Arabic text, enabling it to generate coherent\n",
       "and contextually relevant text.\n",
       "- **Applications**: Excels in tasks like synthetic news generation, conversational agents, and zero-shot question \n",
       "answering. It can produce human-like articles and responses.\n",
       "- **Architecture**: Built on the GPT-\u001b[1;36m2\u001b[0m architecture, which is optimized for predicting the next word in a sequence \n",
       "based on preceding context.\n",
       "\n",
       "### Summary:\n",
       "- **AraBERT** is tailored for understanding and processing Arabic text, making it suitable for comprehension tasks.\n",
       "- **AraGPT2** is optimized for generating text and maintaining context over longer passages, reflecting its \n",
       "strengths in creative writing and conversational applications.\n",
       "\n",
       "Each model serves distinct purposes within Arabic natural language processing, showcasing their respective \n",
       "strengths in understanding versus generation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
