{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will experiment RAG with multi-document agent.\n",
    "\n",
    "- Define a reader to read the `pdf` sample file [AraGPT2](./data/aragpt2.pdf) paper.\n",
    "- Define a `splitter` to process the texts of the document.\n",
    "- Set the LLM embedding and generation model ids.\n",
    "- Create the engines from the Indexes and define a tool wrapper around them.\n",
    "- Create Index for tool objects.\n",
    "- Define the agent worker and agent runner that utilize memory.\n",
    "- Excute the multi-docs agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variables\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some constants\n",
    "GENERATION_MODEL_ID = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL_ID = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "\n",
    "def get_nodes(file_paths: list[str]) -> dict[str, TextNode]:\n",
    "    \"\"\"Extract text nodes from documents.\n",
    "    \n",
    "    inputs:\n",
    "        file_paths (list[str]): paths to pdf files. must be unique.\n",
    "    returns:\n",
    "        nodes_dict (dict[str, TextNode]): mapping of file paths to nodes.\n",
    "    \"\"\"\n",
    "    nodes_dict = {file_path.split(\"/\")[-1].split(\".\")[0]: [] for file_path in file_paths}\n",
    "    documents_reader = SimpleDirectoryReader(input_files=file_paths)\n",
    "    documents = documents_reader.load_data()\n",
    "    sentence_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "    for node in nodes:\n",
    "        nodes_dict[node.metadata[\"file_name\"].split(\".\")[0]].append(node)\n",
    "    return nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/arabert.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/gpt2.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/aragpt2.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/camel_parser.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data/camel_bert.pdf'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'data/arabert.pdf'\u001b[0m, \u001b[32m'data/gpt2.pdf'\u001b[0m, \u001b[32m'data/aragpt2.pdf'\u001b[0m, \u001b[32m'data/camel_parser.pdf'\u001b[0m, \u001b[32m'data/camel_bert.pdf'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "file_paths = glob.glob(\"data/*\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = get_nodes(file_paths=file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dict_keys</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'arabert'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'aragpt2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'camel_parser'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'camel_bert'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'arabert'\u001b[0m, \u001b[32m'gpt2'\u001b[0m, \u001b[32m'aragpt2'\u001b[0m, \u001b[32m'camel_parser'\u001b[0m, \u001b[32m'camel_bert'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nodes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Vector Search and Summary Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "\n",
    "def get_tools_from_nodes(\n",
    "    nodes: dict[str, TextNode]\n",
    ") -> dict[str, list[QueryEngineTool]]:\n",
    "    \"\"\"Define query engine tools from nodes dictionary.\n",
    "\n",
    "    inputs:\n",
    "        nodes (dict[str, TextNode]): text nodes for each document by name.\n",
    "    returns:\n",
    "        tools_dict (dict[str, list[QueryEngineTool]]): tools for each document by name.\n",
    "    \"\"\"\n",
    "    tools_dict = {file_name: [] for file_name in nodes.keys()}\n",
    "    for file_name, text_nodes in nodes.items():\n",
    "        print(f\"Creating tools for file: {file_name}\")\n",
    "        # define vector tool\n",
    "        vector_index = VectorStoreIndex(nodes=text_nodes)\n",
    "        vector_engine = vector_index.as_query_engine()\n",
    "        vector_metadata = ToolMetadata(\n",
    "            name=f\"vector_tool_for_{file_name}\",\n",
    "            description=f\"Useful for retrieving specific context from the {file_name} paper.\",\n",
    "        )\n",
    "        vector_tool = QueryEngineTool(\n",
    "            query_engine=vector_engine, metadata=vector_metadata\n",
    "        )\n",
    "        # define summary tool\n",
    "        summary_index = SummaryIndex(nodes=text_nodes)\n",
    "        summary_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "        summary_metadata = ToolMetadata(\n",
    "            name=f\"summary_tool_for_{file_name}\",\n",
    "            description=f\"Useful for summarization questions related to the {file_name} paper.\",\n",
    "        )\n",
    "        summary_tool = QueryEngineTool(\n",
    "            query_engine=summary_engine, metadata=summary_metadata\n",
    "        )\n",
    "        tools_dict[file_name].extend([vector_tool, summary_tool])\n",
    "    return tools_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: arabert\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: arabert\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: gpt2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: gpt2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: aragpt2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: aragpt2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: camel_parser\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: camel_parser\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating tools for file: camel_bert\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating tools for file: camel_bert\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tools = get_tools_from_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">llama_index.core.tools.query_engine.QueryEngineTool</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe9f4ab9d50</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mllama_index.core.tools.query_engine.QueryEngineTool\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fe9f4ab9d50\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tools[\"arabert\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Objects Vector Search Tool\n",
    "\n",
    "This will do vector search over the tools we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.objects import ObjectIndex, ObjectRetriever\n",
    "\n",
    "\n",
    "def get_object_tool(tools: dict[str, list[QueryEngineTool]]) -> ObjectRetriever:\n",
    "    \"\"\"Define tools query engine tool.\n",
    "\n",
    "    inputs:\n",
    "        tools (dict[str, list[QueryEngineTool]]): tools for each document by name.\n",
    "    returns:\n",
    "        tools_retriever (ObjectRetriever): tools object retriever.\n",
    "    \"\"\"\n",
    "    tools_index = ObjectIndex.from_objects(\n",
    "        objects=[tl for _, tls in tools.items() for tl in tls], index_cls=VectorStoreIndex\n",
    "    )\n",
    "    tools_retriever = tools_index.as_retriever(similarity_top_k=4)\n",
    "    return tools_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">llama_index.core.objects.base.ObjectRetriever</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe9f646a9d0</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mllama_index.core.objects.base.ObjectRetriever\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fe9f646a9d0\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tools_retriever = get_object_tool(tools)\n",
    "print(tools_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Worker and Runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM Backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=GENERATION_MODEL_ID)\n",
    "Settings.embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are an agent designed to answer queries over a set of given papers. Please always use the tools provided to \n",
       "answer a question. Do not rely on prior knowledge.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are an agent designed to answer queries over a set of given papers. Please always use the tools provided to \n",
       "answer a question. Do not rely on prior knowledge.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sys_prompt = \"\"\"\\\n",
    "You are an agent designed to answer queries over a set of \\\n",
    "given papers. Please always use the tools provided to answer \\\n",
    "a question. Do not rely on prior knowledge.\\\n",
    "\"\"\"\n",
    "\n",
    "print(sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import (\n",
    "    AgentRunner,\n",
    "    FunctionCallingAgentWorker,\n",
    ")\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=tools_retriever,\n",
    "    system_prompt=sys_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare between the AraBERT and AraGPT2 models on a high level.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_for_aragpt2 with args: {\"input\": \"AraGPT2 model overview\"}\n",
      "=== Function Output ===\n",
      "The AraGPT2 model is the first advanced Arabic language generation model developed from scratch on a large Arabic corpus of internet text and news articles. It comes in four size variants: base, medium, large, and mega, with the mega model having 1.46 billion parameters, making it the largest Arabic language model available. The model has been evaluated successfully on various tasks such as synthetic news generation and zero-shot question answering. Additionally, it achieved a perplexity of 29.8 on held-out Wikipedia articles and demonstrated the ability to generate news articles that are challenging to distinguish from those written by humans. An automatic discriminator model with a 98% accuracy in detecting model-generated text has also been developed and released alongside the AraGPT2 models.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_for_gpt2 with args: {\"input\": \"AraBERT model overview\"}\n",
      "=== Function Output ===\n",
      "The AraBERT model overview is not provided in the given context information.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_for_aragpt2 with args: {\"input\": \"AraBERT model overview\"}\n",
      "=== Function Output ===\n",
      "The AraBERT model is a transformer-based model that focuses on natural language understanding (NLU) in Arabic. It is pre-trained using the Masked Language Modeling (MLM) task. AraBERT is designed to improve performance in tasks related to NLU in Arabic text.\n",
      "=== LLM Response ===\n",
      "Here's a high-level comparison between the AraBERT and AraGPT2 models:\n",
      "\n",
      "### AraBERT\n",
      "- **Purpose**: Focuses on natural language understanding (NLU) in Arabic.\n",
      "- **Architecture**: Transformer-based model.\n",
      "- **Training Method**: Pre-trained using the Masked Language Modeling (MLM) task.\n",
      "- **Applications**: Designed to improve performance in various NLU tasks related to Arabic text.\n",
      "\n",
      "### AraGPT2\n",
      "- **Purpose**: Advanced Arabic language generation model.\n",
      "- **Architecture**: Developed from scratch on a large Arabic corpus of internet text and news articles.\n",
      "- **Variants**: Comes in four size variants (base, medium, large, and mega), with the mega model having 1.46 billion parameters.\n",
      "- **Performance**: Evaluated on tasks like synthetic news generation and zero-shot question answering, achieving a perplexity of 29.8 on held-out Wikipedia articles.\n",
      "- **Human-like Generation**: Demonstrates the ability to generate news articles that are difficult to distinguish from human-written content.\n",
      "- **Discriminator Model**: An automatic discriminator model with 98% accuracy in detecting model-generated text was developed alongside it.\n",
      "\n",
      "In summary, AraBERT is tailored for understanding Arabic text, while AraGPT2 is geared towards generating Arabic text, showcasing significant capabilities in language generation tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"Compare between the AraBERT and AraGPT2 models on a high level.\"\n",
    "\n",
    "\n",
    "response = agent.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here's a high-level comparison between the AraBERT and AraGPT2 models:\n",
       "\n",
       "### AraBERT\n",
       "- **Purpose**: Focuses on natural language understanding <span style=\"font-weight: bold\">(</span>NLU<span style=\"font-weight: bold\">)</span> in Arabic.\n",
       "- **Architecture**: Transformer-based model.\n",
       "- **Training Method**: Pre-trained using the Masked Language Modeling <span style=\"font-weight: bold\">(</span>MLM<span style=\"font-weight: bold\">)</span> task.\n",
       "- **Applications**: Designed to improve performance in various NLU tasks related to Arabic text.\n",
       "\n",
       "### AraGPT2\n",
       "- **Purpose**: Advanced Arabic language generation model.\n",
       "- **Architecture**: Developed from scratch on a large Arabic corpus of internet text and news articles.\n",
       "- **Variants**: Comes in four size variants <span style=\"font-weight: bold\">(</span>base, medium, large, and mega<span style=\"font-weight: bold\">)</span>, with the mega model having <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.46</span> \n",
       "billion parameters.\n",
       "- **Performance**: Evaluated on tasks like synthetic news generation and zero-shot question answering, achieving a \n",
       "perplexity of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.8</span> on held-out Wikipedia articles.\n",
       "- **Human-like Generation**: Demonstrates the ability to generate news articles that are difficult to distinguish \n",
       "from human-written content.\n",
       "- **Discriminator Model**: An automatic discriminator model with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span>% accuracy in detecting model-generated text was\n",
       "developed alongside it.\n",
       "\n",
       "In summary, AraBERT is tailored for understanding Arabic text, while AraGPT2 is geared towards generating Arabic \n",
       "text, showcasing significant capabilities in language generation tasks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here's a high-level comparison between the AraBERT and AraGPT2 models:\n",
       "\n",
       "### AraBERT\n",
       "- **Purpose**: Focuses on natural language understanding \u001b[1m(\u001b[0mNLU\u001b[1m)\u001b[0m in Arabic.\n",
       "- **Architecture**: Transformer-based model.\n",
       "- **Training Method**: Pre-trained using the Masked Language Modeling \u001b[1m(\u001b[0mMLM\u001b[1m)\u001b[0m task.\n",
       "- **Applications**: Designed to improve performance in various NLU tasks related to Arabic text.\n",
       "\n",
       "### AraGPT2\n",
       "- **Purpose**: Advanced Arabic language generation model.\n",
       "- **Architecture**: Developed from scratch on a large Arabic corpus of internet text and news articles.\n",
       "- **Variants**: Comes in four size variants \u001b[1m(\u001b[0mbase, medium, large, and mega\u001b[1m)\u001b[0m, with the mega model having \u001b[1;36m1.46\u001b[0m \n",
       "billion parameters.\n",
       "- **Performance**: Evaluated on tasks like synthetic news generation and zero-shot question answering, achieving a \n",
       "perplexity of \u001b[1;36m29.8\u001b[0m on held-out Wikipedia articles.\n",
       "- **Human-like Generation**: Demonstrates the ability to generate news articles that are difficult to distinguish \n",
       "from human-written content.\n",
       "- **Discriminator Model**: An automatic discriminator model with \u001b[1;36m98\u001b[0m% accuracy in detecting model-generated text was\n",
       "developed alongside it.\n",
       "\n",
       "In summary, AraBERT is tailored for understanding Arabic text, while AraGPT2 is geared towards generating Arabic \n",
       "text, showcasing significant capabilities in language generation tasks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
